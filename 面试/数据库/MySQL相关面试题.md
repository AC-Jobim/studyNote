# 一、磁盘预读

**磁盘预读：**

* 根据**局部性原理**，**在读取磁盘数据的时候，一般为页（page）的整倍数**。所以即使只需要读取一个字节，磁盘也会读取一页的数据。
* 页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。

**为什么使用B-Tree/B+Tree：**

* 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。即：索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。

* 数据库系统巧妙利用了磁盘预读原理，**将一个节点的大小设为等于一个页（InnoDB数据页大小16KB）**，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧：

  > 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。

* B-Tree中一次检索最多需要h-1次I/O（**根节点常驻内存**），渐进复杂度为O（h）=O（logmN）。一般实际应用中，m是非常大的数字，通常超过100，因此h非常小（通常不超过3）。

* 而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O（h），效率明显比B-Tree差很多。



# 二、MySQL索引的数据结构

**索引是一种用于快速查询和检索数据的数据结构。** 

索引的数据结构和具体存储引擎的实现有关，在MySQL中使用较多的索引有**Hash索引**，**B+树索引**等，而我们经常使用的InnoDB存储引擎的默认索引实现为：B+树索引。

## 2.1 Hash索引和B+树区别

首先要知道Hash索引和B+树索引的底层实现原理：

hash索引底层就是**hash表**，进行查找时，调用一次hash函数就可以获取到相应的键值，之后进行回表查询获得实际数据，因此**在绝大多数需求为单条记录查询的时候**，可以**选择哈希索引**，查询性能最快，Hash 索引不支持顺序和范围查询。

B+树底层实现是**多路平衡查找树**。对于每一次的查询都是从根节点出发，查找到叶子节点方可以获得所查键值，然后根据查询判断是否需要回表查询数据。

- hash索引进行等值查询更快(一般情况下)，但是却**无法进行范围查询和使用索引进行排序**。
  因为在hash索引中经过hash函数建立索引之后，索引的顺序与原顺序无法保持一致，不能支持范围查询。而B+树的的所有节点皆遵循(左节点小于父节点，右节点大于父节点，多叉树也类似)，天然支持范围。

- hash索引**不支持模糊查询以及多列索引的最左前缀匹配**。原理也是因为hash函数的不可预测。AAAA和AAAAB的索引没有相关性。
- hash索引**任何时候都避免不了回表查询数据**，而B+树在符合某些条件(聚簇索引，覆盖索引等)的时候可以只通过索引完成查询。
- hash索引虽然在等值查询上较快，但是**不稳定**。性能不可预测，当某个键值存在大量重复的时候，发生hash碰撞，此时效率可能极差。而**B+树的查询效率比较稳定**，对于所有的查询都是从根节点到叶子节点，且树的高度较低。



> MEMORY存储引擎使用的Hash索引

## 2.2 B+树和B树区别

```
B树和B+树区别：
    1、B 树的所有节点既存放键(key) 也存放数据(data);而 B+树只有叶子节点存放 key 和 data，其他节点只存放 key。
    2、B 树的叶子节点都是独立的;B+树的叶子节点使用双向指针。
    3、B 树的检索的过程相当于对范围内的每个节点的关键字做二分查找，可能还没有到达叶子节点，检索就结束了。而 B+树的检索效率就很稳定了，任何查找都是从根节点到叶子节点的过程，叶子节点的顺序检索很明显。
```

- **B+树的磁盘读写代价更低，IO次数更少：B+树的内部节点并没有指向关键字具体信息的指针，因此其内部节点相对B树更小，如果把所有同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多，一次性读入内存的需要查找的关键字也就越多，相对IO读写次数就降低了。**
- **B+树的查询效率更加稳定**：由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。
- **B+树更便于遍历**：由于B+树的数据都存储在叶子结点中，分支结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更加适合在区间查询的情况，所以通常B+树用于数据库索引。
- **B+树更适合基于范围的查询**：B树在提高了IO性能的同时并没有解决元素遍历的我效率低下的问题，正是为了解决这个问题，B+树应用而生。B+树只需要去遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作或者说效率太低。

但是B树也有优点，B树可以在内部节点同时存储键和值，因此，把频繁访问的数据放在靠近根节点的地方将会大大提高热点数据的查询效率。这种特性使得B树在特定数据重复多次查询的场景中更加高效。



**B树图解：**

![image-20210924123815765](https://gitee.com/jobim/blogimage/raw/master/img/20210924123815.png)

**B+树图解：**

![image-20210924123833871](https://gitee.com/jobim/blogimage/raw/master/img/20210924123833.png)



**为什么mysql不使用红黑树/平衡树？**

- 数据库设计利用了**磁盘预读原理**，将**一个节点的大小设为等于一个页**，这样每个节点只需要一次 I/O 就可以完全载入。而红黑树/平衡树只有两个节点，不能把页充分的利用，而已因为旋转操作，不能把父子节点放到一个页中。会因为树的深度过深而造成io次数变多。
- 在增删改查的过程中，时间复杂度为log2n，而b+是logmN，体现在内存和磁盘的IO上，代价比较大。
- 特别是插入的时候，**红黑树需要进行节点颜色调整以及平衡操作（平衡需要加锁），对于频繁的插入而言，这是一个耗时的过程**，而b+树仅仅是页分裂问题。另外从并发交付来说，要么线程不安全，要么加锁虽然线程安全，但是牺牲了效率。
- **区间查找**而言，b+树的叶子节点带有双向链表，因此比较方便。对于红黑树而言，虽然可以实现，比如通过中序遍历的方式，但是实现起来比较复杂



**B树的插入操作：**

* 插入操作是指插入一条记录，即（key, value）的键值对。如果B树中已存在需要插入的键值对，则用需要插入的value替换旧的value。若B树不存在这个key,则一定是在叶子结点中进行插入操作。
  * 根据要插入的key的值，找到叶子结点并插入。
  * 判断当前结点key的个数是否小于等于m-1，若满足则结束，否则进行第3步。
  * 以结点中间的key为中心分裂成左右两部分（当前节点个数key大于m-1），然后将这个中间的key插入到父结点中，这个key的左子树指向分裂后的左半部分，这个key的右子支指向分裂后的右半部分，然后将当前结点指向父结点，继续进行第3步。



## 2.3 聚簇和非聚簇索引的区别？

[聚簇索引与非聚簇索引（也叫二级索引）--最清楚的一篇讲解](https://cloud.tencent.com/developer/article/1541265)

都是B+树的数据结构

- **聚簇索引**：**将数据存储与索引放到了一块、并且是按照一定的顺序组织的，找到索引也就找到了数据**，数据的物理存放顺序与索引顺序是一致的，即：只要索引是相邻的，那么对应的数据一定也是相邻地存放在磁盘上的。
- **非聚簇索引**：叶子节点不存储数据、存储的是**数据行地址**，也就是说根据索引查找到数据行的位置再取磁盘查找数据。（myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因 ）



**聚簇索引的优势：**

1. 由于行数据和叶子节点存储在一起，同一页中会有多条行数据，访问同一数据页不同行记录时，已经把页加载到了Buffer中，**再次访问的时候，会在内存中完成访问，不必访问磁盘**。**这样主键和行数据是一起被载入内存的，找到叶子节点就可以立刻将行数据返回了**，如果按照主键Id来组织数据，获得数据更快。
2. 辅助索引使用主键作为"指针"而不是使用地址值作为指针的好处是，**减少了当出现行移动或者数据页分裂时辅助索引的维护工作，使用主键值当作指针会让辅助索引占用更多的空间，换来的好处是InnoDB在移动行时无须更新辅助索引中的这个"指针"**。也就是说行的位置（实现中通过16K的Page来定位）会随着数据库里数据的修改而发生变化（前面的B+树节点分裂以及Page的分裂），**使用聚簇索引就可以保证不管这个主键B+树的节点如何变化，辅助索引树都不受影响。**
3. 聚簇索引适合用在排序的场合， 因为聚簇索引本身已经是按照物理顺序放置的，排序很快 。非聚簇索引不适合
4. 取出一定范围数据的时候，使用用聚簇索引
5. 二级索引需要两次索引查找，而不是一次才能取到数据，因为存储引擎第一次需要通过二级索引找到索引的叶子节点，从而找到数据的主键，然后在聚簇索引中用主键再次查找索引，再找到数据





**聚簇索引的劣势：**

1. **维护索引很昂贵，特别是插入新行或者主键被更新导至要分页(page split)的时候**。建议在大量插入新行后，选在负载较低的时间段，通过OPTIMIZE TABLE优化表，因为必须被移动的行数据可能造成碎片。使用独享表空间可以弱化碎片
2. 表因为使用UUId（随机ID）作为主键，使数据存储稀疏，这就会出现聚簇索引有可能有比全表扫面更慢，所以建议使用int的auto_increment作为主键。
3. **如果主键比较大的话，那辅助索引将会变的更大**，因为辅助索引的叶子存储的是主键值；过长的主键值，会导致非叶子节点占用占用更多的物理空间



**聚簇索引图解：**

![image-20210924124010987](https://gitee.com/jobim/blogimage/raw/master/img/20210924124011.png)

**非聚簇索引图解：**

![image-20210924124052919](https://gitee.com/jobim/blogimage/raw/master/img/20210924124053.png)





## 2.4 InnoDB和MyISAM的区别

1. 主索引的区别， **InnoDB 表是基于聚簇索引建立的** ，数据文件本身就是索引文件。而MyISAM的索引和数据是分开的。

2. 辅助索引的区别：InnoDB的辅助索引data域存储相应记录主键的值而不是地址。而MyISAM的辅助索引和主索引没有多大区别。

**InnoDB中有主键，主键一定是聚簇索引**，不手动设置、则会使用第一个非空的unique列作为聚簇索引，没有unique列，则会使用数据库内部的一个行的隐藏id来当作聚簇索引。在聚簇索引之上创建的索引称之为辅助索引，**辅助索引访问数据总是需要二次查找**，非聚簇索引都是辅助索引，像复合索引、前缀索引、唯一索引，辅助索引叶子节点存储的不再是行的物理位置，而是主键值。

**MyISM使用的是非聚簇索引**，没有聚簇索引，主键索引B+树的节点存储了主键，辅助键索引B+树存储了辅助键。表数据存储在独立的地方，**这两颗B+树的叶子节点存储的都是数据行地址**。对于表数据来说，这两个键没有任何差别。如果涉及到大数据量的排序、全表扫描、count之类的操作的话，还是MyISAM占优势些，因为索引所占空间小，这些操作是需要在内存中完成的。



3. **InnoDB为支持行锁和表锁，MyISAM只支持表锁**
4. **MyISAM不支持外键和事务。InnoDB支持外键和事务**



**回表查询**，即现在辅助索引中定位主键值，再定位行记录，它的性能较扫一遍索引树更低 。



<font color='blue' size='4'>**mysql一棵B+树能存多少条数据？**</font>

* InnoDB一个数据页大小16KB。
* 假设主键ID为`bigint`类型，长度为`8字节`，而指针大小在InnoDB源码中设置为`6字节`，这样一共`14字节`。

* 那么一个页中能存放多少这样的组合，就代表有多少指针，即 `16384 / 14 = 1170`
* 对于第三层，因为innodb的叶子节点，是直接包含整条mysql数据的，如果字段非常多的话数据所占空间是不小的，我们这里以1kb计算，所以在第三层，每个节点为16kb，那么每个节点是可以放16个数据的
* 高度为3的B+树可以存放的行数 = `1170 * 1170 * 16 = 21902400`

# 三、索引设计的原则

查询更快、占用空间更小

1. **为经常需要<font color='red'>排序、分组和联合操作</font>的字段**建立索引，为常作为**<font color='red'>查询条件的字段建立索引</font>**
2. 使用**短索引**，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间，
   如果搜索词超过索引前缀长度，则使用索引排除不匹配的行，然后检查其余行是否可能匹配。
3. 创建索引的时候应**遵守最左前缀原则** 

4. 不要过度创建索引。**索引需要额外的磁盘空间，并降低写操作的性能**。在修改表内容的时候，索引会进
   行更新甚至重构，索引列越多，这个时间就会越长。所以只保持需要的索引有利于查询即可。
5. 定义有外键的数据列一定要建立索引。
6. **更新频繁字段不适合创建索引**
7. **基数较小的表，索引效果较差**，没有必要在此列建立索引
8. 对于那些**不被经常查询**的字段，**重复值比较多的列**不要建立索引。
9. 对于定义为text、image和bit的数据类型的列不要建立索引。

<font color='blue' size='4'>**索引分析：**</font>

* 对于链接查询，**永远都是小的结果集驱动大的结果集**。（小表驱动大表）
* **左连接将索引创建在右表上更合适，右连接将索引创建在左表上更合适**
* **最佳左前缀法则：如果索引是多字段的复合索引，要遵守`最佳左前缀法则`。指的是查询从索引的最左前列开始并且不跳过索引中的字段**
* **不在索引列上做任何操作**（计算、函数、(自动or手动)类型转换），会导致索引失效而转向全表扫描
* **查询范围的字段使用到了索引，范围之后的索引字段会失效**
* **尽量使用覆盖索引**
* mysql在使用不等于（!=或者<>）的时候会导致索引失效
* is null、is not null也无法使用索引
* 模糊查询时，尽量不要把%通配符写在开头。只有当百分号在右边，索引才会生效。（注意：当百分号在右边时，模糊查询之后的字段索引也会被用到）
* **字符串不加单引号索引失效**
* 少用or，用它来连接时会索引失效（高版本有优化）
* in使用关键字少的时候，会走索引的，**但是内容较多，就会造成索引失效**

![image-20211014111859084](https://gitee.com/jobim/blogimage/raw/master/img/20211014111906.png)



**联合索引图解：**

![image-20210924125449916](https://gitee.com/jobim/blogimage/raw/master/img/20210924125450.png)





# 四、MySQL中的锁

## 4.1 MySQL锁的分类

按照**锁的粒度**把数据库锁分为行级锁(INNODB引擎)、表级锁(MYISAM引擎)和页级锁(BDB引擎 )。

- **行锁**

  Mysql中锁定 **粒度小** 的一种锁，只针对当前操作的行进行加锁。 **行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。**  

- **表锁**

  Mysql中锁定 **粒度大** 的一种锁，对当前操作的整张表加锁，实现简单 **，资源消耗也比较少，加锁快，不会出现死锁** 。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM和 InnoDB引擎都支持表级锁。

* 页级锁 

  页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般



**从锁的类别上来讲**：有共享锁和排他锁。

* **共享锁 （Share Locks，简记为S ）**: 又叫做读锁。 当用户要进行数据的读取时，对数据加上共享锁。**共享锁可以同时加上多个**。

* **排他锁 (Exclusive lock,简记为X锁)** : 又叫做写锁。 当用户要进行数据的写入时，对数据加上排他锁。**排他锁只可以加一个**，他和其他的排他锁，共享锁都相斥。



**虽然使用行级锁具有粒度小、并发度高等特点，但是表级锁有时候也是非常必要的**：

- 事务更新大表中的大部分数据直接使用表级锁效率更高；
- 事务比较复杂，使用行级索很可能引起死锁导致回滚。



**MySQL中InnoDB引擎的行锁是怎么实现的？**

```
答：InnoDB是基于索引来完成行锁

例: select * from tab_with_index where id = 1 for update;

for update 可以根据条件来完成行锁锁定，并且 id 是有索引键的列，如果 id 不是索引键那么InnoDB将完成表锁，并发将无从谈起
```





间隙锁：

[MySQL的锁机制 - 记录锁、间隙锁、临键锁 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/48269420)





## 4.2 什么是死锁？怎么解决？

死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。

**常见的解决死锁的方法：**

1. 如果不同程序会并发存取多个表，尽量**约定以相同的顺序访问表**，可以大大降低死锁机会。
2. 在同一个事务中，**尽可能做到一次锁定所需要的所有资源**，减少死锁产生概率；
3. 对于非常容易产生死锁的业务部分，可以**尝试使用升级锁定颗粒度**，通过表级锁定来减少死锁产生的概率；



# 五、EXPLAIN执行计划

EXPLAIN：**SQL的执行计划**，使用EXPLAIN关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是如何处理SQL语句的。

EXPLAIN能干嘛？

>可以查看以下信息：
>
>- `id`：**表的读取顺序。**
>- `select_type`：**数据查询的类型，主要是用于区别普通查询、联合查询、子查询等的复杂查询。**
>- `table`：这个数据是基于哪张表的
>- `type`：**查询的访问类型**
>- `possible_keys`：**哪些索引可以使用。**
>- `key`：**哪些索引被实际使用。**
>- `ref`：表之间的引用。
>- `rows`：**每张表有多少行被优化器查询。**
>
>* `Extra`：其他的额外重要的信息。

**EXPLAIN字段：**

1. **id：表的读取和加载顺序**
   id是select查询的序列号，包含一组数字，表示查询中执行select子句或操作表的顺序

   ![image-20210915100527925](https://gitee.com/jobim/blogimage/raw/master/img/20210915100528.png)

  值有以下三种情况：

  > * id相同，执行顺序由上至下。
  >
  > * id不同，如果是子查询，id的序号会递增，**id值越大优先级越高，越先被执行。**
  > * id相同不同，同时存在。**永远是id大的优先级最高，id相等的时候顺序执行。**



2. **select_type：数据查询的类型，主要是用于区别，普通查询、联合查询、子查询等的复杂查询**

   ![image-20210915100552063](https://gitee.com/jobim/blogimage/raw/master/img/20210915100552.png)

   >- `SIMPLE`：简单的`SELECT`查询，查询中不包含子查询或者`UNION `。
   >- `PRIMARY`：查询中如果包含任何复杂的子部分，最外层查询则被标记为`PRIMARY`。
   >- `SUBQUERY`：在`SELECT`或者`WHERE`子句中包含了子查询。
   >- `DERIVED`：在`FROM`子句中包含的子查询被标记为`DERIVED(衍生)`，MySQL会递归执行这些子查询，把结果放在临时表中。
   >- `UNION`：如果第二个`SELECT`出现在`UNION`之后，则被标记为`UNION`；若`UNION`包含在`FROM`子句的子查询中，外层`SELECT`将被标记为`DERIVED`。
   >- `UNION RESULT`：从`UNION`表获取结果的`SELECT`。

3. **table：表的来源**
   table表示这个数据是基于哪张表的


4. **type：访问类型排列，扫描方式**
   
   ![image-20210915100623995](https://gitee.com/jobim/blogimage/raw/master/img/20210915100624.png)
   
   type 是查询的访问类型。是较为重要的一个指标，结果值从最好到最坏依次是：`system`>`const`>`eq_ref`>`ref`>`range`>`index`>`ALL`。除了`ALL`没有用到索引，其他级别都用到索引了。
   
   >- `system`：表只有一行记录（等于系统表），这是`const`类型的特例，平时不会出现，这个也可以忽略不计。
   >- `const`：表示通过索引一次就找到了，`const`用于比较`primary key`或者`unique`索引。因为只匹配一行数据，所以很快。如将主键置于`where`列表中，MySQL就能将该查询转化为一个常量。
   >- `eq_ref`：唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配。（在联表查询中使用primary key或者unique key做为关联条件）
   >
   >- `ref`：非唯一性索引扫描，返回匹配某个单独值的所有行。本质上也是一种索引访问，它返回所有匹配某个单独值的行， 然而，它可能会找到多个符合条件的行，所以他应该属于查找和扫描的混合体
   >
   >- `range`：只检索给定范围的行，一般就是在`WHERE`语句中出现了`BETWEEN`、`< >`、`in`等的查询。这种范围扫描索引比全表扫描要好，因为它只需要开始于索引树的某一点，而结束于另一点，不用扫描全部索引。
   >
   >- `index`：`Full Index Scan`，全索引扫描，`index`和`ALL`的区别为`index`类型只遍历索引树。**也就是说虽然`ALL`和`index`都是读全表，但是`index`是从索引中读的，`ALL`是从磁盘中读取的。**
   >- `ALL`：`Full Table Scan`，没有用到索引，全表扫描。
   >
   >一般来说，得保证查询至少达到`range`级别，最好达到`ref`。
   
5. **possible_keys**
   `possible_keys`：显示可能应用在这张表中的索引，一个或者多个。查询涉及到的字段上若存在索引，则该索引将被列出，**但不一定被查询实际使用。**

6. **key** 
   `key`：实际使用的索引。如果为`NULL`，则没有使用索引或索引失效。查询中如果使用了覆盖索引，则该索引仅仅出现在`key`列表中。
   ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210402160557875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDYzMDY1Ng==,size_16,color_FFFFFF,t_70)


7. **key_len**
   `key_len`：表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度。
   `key_len`显示的值为索引字段的最大可能长度，并非实际使用长度，即`key_len`是根据表定义计算而得，不是通过表内检索出的。在不损失精度的情况下，长度越短越好。
   key_len计算规则：[https://blog.csdn.net/qq_34930488/article/details/102931490](https://blog.csdn.net/qq_34930488/article/details/102931490)

8. **ref**
   `ref`：显示索引的哪一列被使用了，如果可能的话，是一个常数。哪些列或常量被用于查找索引列上的值。

9. **rows**
   `rows`：根据表统计信息及索引选用情况，**大致估算出找到所需的记录需要读取的行数。**

10. **Extra**
    `Extra`：包含不适合在其他列中显示但十分重要的额外信息。

>- `Using filesort`：说明MySQL会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。**MySQL中无法利用索引完成的排序操作成为"文件排序"。**
>- `Using temporary`：使用了临时表保存中间结果，MySQL在対查询结果排序时使用了临时表。常见于排序`order by`和分组查询`group by`。**临时表対系统性能损耗很大。**
>- `Using index`：表示相应的`SELECT`操作中使用了覆盖索引，避免访问了表的数据行，效率不错！如果同时出现`Using where`，表示索引被用来执行索引键值的查找；如果没有同时出现`Using where`，表明索引用来读取数据而非执行查找动作。
>![在这里插入图片描述](https://img-blog.csdnimg.cn/20210402192334432.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDYzMDY1Ng==,size_16,color_FFFFFF,t_70)
>- `Using where`：表明使用了`WHERE`过滤。
>- `Using join buffer`：使用了连接缓存。
>- `impossible where`：`WHERE`子句的值总是false，不能用来获取任何元组。





# 六、事务的基本特性和隔离级别



**事务的含义**：

* 一个或一组sql语句组成一个执行单元，这个执行单元要么全部执行，要么全部不执行。



**事务的四大特性（ACID）：**

1. **原子性**（Atomicity）
   原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。
2. **一致性**（Consistency）
   执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的。
3. **隔离性**（Isolation）
   并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的。
4. **持久性**（Durability）
   一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。



**什么是脏读？幻读？不可重复读？**

* 脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。
* 不可重复读(Non-repeatable read)：在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。
* 幻读(Phantom Read)：在一个事务的两次查询中数据数量不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。



**事务的隔离级别：**

* ![image-20211012152117805](https://gitee.com/jobim/blogimage/raw/master/img/20211012152117.png)
* 注意的是：Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别 





## 6.1 ACID的底层实现原理

<font color='blue' size='4'>**原子性：**</font>

**undo log（回滚日志）**实现。实现原子性的关键，是当事务回滚时能够撤销所有已经成功执行的sql语句。InnoDB实现回滚，靠的是undo log：当事务对数据库进行修改时，InnoDB会生成对应的undo log；如果事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子（执行相反的操作）。

* undo log属于逻辑日志，它记录的是sql执行相关的信息。当发生回滚时，InnoDB会根据undo log的内容做与之前相反的工作：对于每个insert，回滚时会执行delete；对于每个delete，回滚时会执行insert；对于每个update，回滚时会执行一个相反的update，把数据改回去。

* 以update操作为例：当事务执行update时，其生成的undo log中会包含被修改行的主键（以便知道修改了哪些行)、修改了哪些列、这些列在修改前后的值等信息，回滚时便可以使用这些信息将数据还原到update之前的状态。



**undo log的作用**

* 用于事务的回滚（保证原子性）；

* 用于MVCC（保证一致性）；



<font color='blue' size='4'>**持久性：**</font>

InnoDB作为mysql的存储引擎，数据是存放在磁盘中的，但是每次读写数据需要磁盘IO，效率就很低。为此，InnoDB提供了缓存（Buffer Pool），Buffer Pool中包含了磁盘中部分数据页的映射，作为访问数据库的缓冲：

![image-20211018094445080](https://gitee.com/jobim/blogimage/raw/master/img/20211018094445.png)

* 读取数据时，首先从Buffer Pool中读取，如果Buffer Pool中没有，则加载磁盘中的数据到Buffer Pool中；

* 写入数据的时候，会首先写入Buffer Pool，Buffer Pool中修改的数据会定期刷新到磁盘（这一过程被称为刷脏）。

**redo log如何保证数据不丢失？**

* 在修改数据时，除了修改Buffer Pool中的数据，还会在redo log Buffer(内存)记录这次操作。当事务提交时，会调用fsync对redo log（磁盘）进行刷盘。重启时可以读取磁盘上的redo log中的数据，对数据库进行恢复。redo log采用的是WAL技术（Write-ahead logging，预写式日志）。所有修改先写入日志（redo log），在更新Buffer Pool，保证数据不会因为mysql宕机而丢失。
* redo log只要写入磁盘的数据，都会从redo log中抹除，数据库重启后，直接将redo log的数据恢复到内存

**redo log也是写磁盘，比BufferPool写入磁盘优点是什么？**

* redo log也需要在事务提交时将日志写入磁盘，但是它要比Buffer Pool中修改的数据写入磁盘（即刷脏）要快：
  * 刷脏是随机IO，每次修改数据位置都是随机，写**redo log是追加操作，属于顺序IO**；
  * 刷脏是以数据页（Page）为单位，Mysql默认页大小为16KB，一个Page上一个小修改都要整页写入，而redo log中是精简的日志数据，无效IO大大减少。

**Redo 的整体流程**

![image-20211018095629608](https://gitee.com/jobim/blogimage/raw/master/img/20211018095629.png)

1. 先将原始数据从磁盘中读入到内存（Buffer Pool）中，修改内存拷贝；
2. 生成redo log并写入`redo log buffer（内存）`，记录的是数据被修改后的值；
3. 当事务commit时，将redo log中的内容刷新到`redo log file（磁盘）`，对redo log file采用追加写的方式；
4. 定期将内存（Buffer Pool）中修改的值刷新到磁盘；



**重要的参数：**

* MySQL支持用户自定义在commit时如何将log buffer中的日志刷log file中。这种控制通过变量 innodb_flush_log_at_trx_commit 的值来决定。该变量有3种值：0、1、2，默认为1。但注意，这个变量只是控制commit动作是否刷新log buffer到磁盘

![image-20211018100444917](https://gitee.com/jobim/blogimage/raw/master/img/20211018100444.png)





<font color='blue' size='4'>**隔离性：**</font>

- 写-写操作：锁机制保证隔离性
- 写-读操作：MVCC保证隔离性



# 七、MyISAM和InnoDB的区别

参考博客：[MyISAM与InnoDB 的区别](https://blog.csdn.net/qq_35642036/article/details/82820178)

1. **InnoDB支持事务**，**MyISAM不支持**，对于InnoDB每一条SQL语言都默认封装成事务，自动提交（**查询速度慢**），这样会影响速度，所以最好把多条SQL语言放在begin和commit之间，组成一个事务。

2. **InnoDB支持外键**，而**MyISAM不支持外键**。对一个包含外键的InnoDB表转为MYISAM会失败。

3. InnoDB**主键索引使用聚集索引**，使用B+Tree作为索引结构，**数据文件是和（主键）索引绑在一起**的（表数据文件本身就是按B+Tree组织的一个索引结构），必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。

   MyISAM是**非聚集索引**，也是使用B+Tree作为索引结构，**索引和数据文件是分离的**，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。

   即：InnoDB的B+树主键索引的叶子节点就是数据文件，辅助索引的叶子节点是主键的值；而MyISAM的B+树主键索引和辅助索引的叶子节点都是数据文件的地址指针。

4. **MyISAM用一个变量保存了整个表的行数**，执行上述语句时只需要读出该变量即可，速度很快（注意不能加有任何WHERE条件）；而**InnoDB不保存表的具体行数**，执行select count(*) from table时需要全表扫描

5. **InnoDB支持表、行(默认)级锁， ** **而MyISAM支持表级锁**。注意：InnoDB的行锁是实现在索引上的

6. **Innodb存储文件有表定义文件（.frm）、数据文件（.ibd）**；而**Myisam是表定义文件（.frm）、数据文件(.MYD)和索引文件（MYI）**



# 九、数据库的三大范式

1. **第一范式**：要求数据库表中的所有字段值都是不可分解的原子值 

   **举例：**

   ![image-20210808212355721](https://gitee.com/jobim/blogimage/raw/master/img/20210808212355.png)

   在上面的表中，**“家庭信息”和“学校信息”列均不满足原子性的要求，故不满足第一范式**，调整如下：

   ![image-20210808212426702](https://gitee.com/jobim/blogimage/raw/master/img/20210808212426.png)



2. **第二范式**：在第一范式的基础上，非主键列完全依赖于主键，而**不能是依赖于主键的一部分**。（**针对于联合主键而言**）

   **举例：**

   ![image-20210808212736270](https://gitee.com/jobim/blogimage/raw/master/img/20210808212736.png)

   在上图所示的情况中，同一个订单中可能包含不同的产品，因此主键必须是“订单号”和“产品号”联合组成，但可以发现，产品数量、产品折扣、产品价格与“订单号”和“产品号”都相关，**但是订单金额和订单时间仅与“订单号”相关，与“产品号”无关**，这样就不满足第二范式的要求，调整如下，需分成两个表：

   ![image-20210808212814188](https://gitee.com/jobim/blogimage/raw/master/img/20210808212814.png)

3. **第三范式**：在第二范式的基础上，非主键列只依赖于主键，**不依赖于其他非主键**（消除传递依赖）。或者：**需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关**

   **举例1：**

   ![image-20210808213243642](https://gitee.com/jobim/blogimage/raw/master/img/20210808213243.png)

   上表中，所有属性都完全依赖于学号，所以满足第二范式，但是“班主任性别”和“班主任年龄”直接依赖的是“班主任姓名”，而不是直接依赖于主键“学号”，它是通过传递才依赖于主键，所以不符合 3NF。所以做如下调整：

   ​	![image-20210808213252146](https://gitee.com/jobim/blogimage/raw/master/img/20210808213252.png)

   **举例2：**

   ![image-20210808214323208](https://gitee.com/jobim/blogimage/raw/master/img/20210808214323.png)

   上面表中“部门名称”和“员工编号”不会直接相关，而是，“员工编号”和“部门编号”、“部门编号”和“部门名称”直接相关。此时会**带来的问题**：

    	1. 数据冗余：“部门名称”多次重复出现。
    	2. 插入异常：组建一个新部门时没有员工信息，也就无法单独插入部门信息。就算强行插入部门信息，员工表中没有员工信息的记录同样是非法记录。
    	3. 删除异常：删除员工信息会连带删除部门信息导致部门信息意外丢失。
    	4. 更新异常：哪怕只修改一个部门的名称也要更新多条员工记录。

   **正确的做法**是：把上表拆分成两张表，以外键形式关联：

   ![image-20210808214608421](https://gitee.com/jobim/blogimage/raw/master/img/20210808214608.png)







# 十、分库分表

好的博客：[面试官：“谈谈分库分表吧？” ](https://baijiahao.baidu.com/s?id=1622441635115622194&wfr=spider&for=pc)



**什么是分库分表？**

* 分库分表就是按照一定的规则，对原有的数据库和表进行拆分，把原本存储于一个库的数据分块存储到多个库上，把原本存储于一个表的数据分块存储到多个表上。
* 业务量剧增，单库数据量越来越大，给存储造成巨大压力。此时数据库就需要相关的优化方案。分库分表就是其中的一种。



**分库分表的方式：**

* **垂直分库/分表**
    > **垂直划分数据库是根据业务进行划分**，例如将shop库中涉及商品、订单、用户的表分别划分出成一个库，通过降低单库（表）的大小来提高性能，但这种方式并没有解决高数据量带来的性能损耗。
    >
    > ![image-20210904151702319](https://gitee.com/jobim/blogimage/raw/master/img/20210904151702.png)
    >
    > **垂直分表的情况就是将一个大表根据业务功能拆分成一个个子表**，例如用户表可根据业务分成基本信息表和详细信息表等。
    >
    > ![image-20210904151727060](https://gitee.com/jobim/blogimage/raw/master/img/20210904151727.png)
    
* **垂直分库/分表的优缺点**

  优点：

  * 拆分后业务清晰，达到专库专用。
  * 数据维护简单、按业务不同业务放在不同机器上

  缺点：
  * 不能解决数据量大带来的性能损耗，读写压力依旧很大
  * 不同的业务无法跨库关联（join），只能通过业务来关联



* **水平分库/分表**

  > **保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量**
  >
  > 注意：分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 **水平拆分最好分库** 。
  >
  > ![image-20210904152441069](https://gitee.com/jobim/blogimage/raw/master/img/20210904152441.png)
  >
  > ![image-20210904152450998](https://gitee.com/jobim/blogimage/raw/master/img/20210904152451.png)

* **水平分库/分表的优缺点**

  优点：

  * 单库（表）的数据量得以减少，有助于提高性能
  * 提高了系统的稳定性和负载能力
  * 切分出的表结构相同，程序改动较少

  缺点：
  * 拆分规则较难抽象
  * 数据分片在扩容时需要迁移
  * 依然存在跨库无法join等问题，同时涉及分布式事务，数据一致性等问题。





# 十一、MVCC解析

> 好的博客：[正确的理解MySQL的MVCC及实现原理](https://blog.csdn.net/SnailMann/article/details/94724197?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-6.baidujs&dist_request_id=&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-6.baidujs)
>
> [通俗易懂数据库MVCC讲解，后悔没早点学 ](https://mp.weixin.qq.com/s/oOL4yradD5w73VsrfoyneA)

## 11.1 MVCC介绍

MVCC（Multi-Version Concurrency Control）即**多版本并发控制**，读取数据时通过一种类似快照的方式（**版本链**）将数据保存下来，这样**读锁就和写锁不冲突了**。通俗讲就是“通过多个版本的记录来实现并发控制” 。不同的事务session会看到自己特定版本的数据，版本链。

> **MVCC只在 READ COMMITTED 和 REPEATABLE READ 两个隔离级别下工作**。其他两个隔离级别够和MVCC不兼容, 因为 READ UNCOMMITTED 总是读取最新的数据行, 而不是符合当前事务版本的数据行。而 SERIALIZABLE 则会对所有读取的行都加锁。



**MVCC 带来的好处是？**

* 多版本并发控制（MVCC）是一种用来**解决读-写冲突的无锁并发控制**，也就是为事务分配单向增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。 所以 MVCC 可以为数据库解决以下问题
  * 在并发读写数据库时，可以做到**在读操作时不用阻塞写操作，写操作也不用阻塞读操作**，提高了数据库并发读写的性能
  * 同时还可以解决脏读，幻读，不可重复读等事务隔离问题，但不能解决更新丢失问题



**当前读和快照读?**

* **当前读**

  它读取的数据库记录，都是`当前最新`的`版本`，会对当前读取的数据进行`加锁`，防止其他事务修改数据。是`悲观锁`的一种操作。

  如下操作都是当前读：

  * select lock in share mode (共享锁)
  * select for update (排他锁)
  * update (排他锁) 
  * insert (排他锁)
  * delete (排他锁)
  * 串行化事务隔离级别

* **快照读**

  快照读的实现是基于`多版本`并发控制，即MVCC，既然是多版本，那么快照读读到的数据不一定是当前最新的数据，有可能是之前`历史版本`的数据。

  如下操作是快照读：

  * 不加锁的select操作（注：事务级别不是串行化）



## 11.2 MVCC的原理

MVCC的目的就是多版本并发控制，在数据库中的实现，就是为了解决`读写冲突`，它的实现原理主要是依赖记录中的 **`版本链`**，**`undo日志`** ，**`Read View`** 来实现的 



### 11.2.1 版本链

我们数据库中的每行数据，除了我们肉眼看见的数据，还有几个`隐藏字段`，得开`天眼`才能看到。分别是`db_trx_id`、`db_roll_pointer`、`db_row_id`。

- `db_trx_id`（最近修改的事务ID）

  6byte，最近修改(修改/插入)`事务ID`：记录`创建`这条记录/`最后一次修改`该记录的`事务ID`。

- `db_roll_pointer`（回滚指针）

  7byte，`回滚指针`，指向`这条记录`的`上一个版本`（存储于rollback segment里）

- `db_row_id`

  6byte，隐含的`自增ID`（隐藏主键），如果数据表`没有主键`，InnoDB会自动以db_row_id产生一个`聚簇索引`。

- 实际还有一个`删除flag`隐藏字段, 记录被`更新`或`删除`并不代表真的删除，而是`删除flag`变了

> ![image-20210917130903367](https://gitee.com/jobim/blogimage/raw/master/img/20210917130903.png)
>
> 如上图，`db_row_id`是数据库默认为该行记录生成的`唯一隐式主键`，`db_trx_id`是当前操作该记录的`事务ID`，而`db_roll_pointer`是一个`回滚指针`，用于配合`undo日志`，指向上一个`旧版本`。



### 11.2.2 **undo log 日志**

Undo log 主要用于`记录`数据被`修改之前`的日志，在表信息修改之前先会把数据拷贝到`undo log`里。

当`事务`进行`回滚时`可以通过undo log 里的日志进行`数据还原`。

**Undo log 的用途**

- 保证`事务`进行`rollback`时的`原子性和一致性`，当事务进行`回滚`的时候可以用undo log的数据进行`恢复`。
- 用于MVCC`快照读`的数据，在MVCC多版本控制中，通过读取`undo log`的`历史版本数据`可以实现`不同事务版本号`都拥有自己`独立的快照数据版本`。

**undo log主要分为两种：**

- `insert undo log`

  代表事务在insert新记录时产生的undo log , 只在事务回滚时需要，并且在事务提交后可以被立即丢弃

- `update undo log`（主要）

  事务在进行update或delete时产生的undo log ; 不仅在事务回滚时需要，在快照读时也需要；

  所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被purge线程统一清除

> purge
>
> * 从前面的分析可以看出，为了实现 InnoDB 的 MVCC 机制，更新或者删除操作都只是设置一下老记录的 deleted_bit ，并不真正将过时的记录删除。
>
> * 为了节省磁盘空间，InnoDB 有专门的 purge 线程来清理 deleted_bit 为 true 的记录。为了不影响 MVCC 的正常工作，purge 线程自己也维护了一个read view（这个 read view 相当于系统中最老活跃事务的 read view ）;如果某个记录的 deleted_bit 为 true ，并且 DB_TRX_ID 相对于 purge 线程的 read view 可见，那么这条记录一定是可以被安全清除的。



**undo log执行流程演示：**

1. 比如一个**有个事务插入 person 表插入了一条新记录**，记录如下，name 为 Jerry , age 为 24 岁，隐式主键是 1，事务 ID和回滚指针，我们假设为 NULL

   ![image-20210918132218787](https://gitee.com/jobim/blogimage/raw/master/img/20210918132218.png)



2. **现在来了一个`事务1`对该记录的 name 做出了修改，改为 Tom**

   * 事务 1修改该行(记录)数据时，数据库会先对该行加**排他锁**
   * **然后把该行数据拷贝到 undo log 中，作为旧记录，既在 undo log 中有当前行的拷贝副本**
   * 拷贝完毕后，修改该行name为Tom，并且修改隐藏字段的事务 ID 为当前事务 1的 ID, 我们默认从 1 开始，之后递增，**回滚指针指向拷贝到 undo log 的副本记录**，既表示我的上一个版本就是它
   * 事务提交后，释放锁

   ![image-20210918132236100](https://gitee.com/jobim/blogimage/raw/master/img/20210918132236.png)



3. **又来了个事务 2修改person 表的同一个记录，将age修改为 30 岁**

   * 在事务2修改该行数据时，数据库也先为**该行加锁**
   * **然后把该行数据拷贝到 undo log 中，作为旧记录，发现该行记录已经有 undo log 了，那么最新的旧数据作为链表的表头，插在该行记录的 undo log 最前面**
   * 修改该行 age 为30岁，并且修改隐藏字段的事务 ID 为当前事务 2的 ID, 那就是 2 ，**回滚指针指向刚刚拷贝到 undo log 的副本记录**
   * 事务提交，释放锁


   ![image-20210918132245840](https://gitee.com/jobim/blogimage/raw/master/img/20210918132245.png)

> 从上面，我们就可以看出，不同事务或者相同事务的对同一记录的修改，会导致该记录的undo log成为一条记录版本线性表，既链表，undo log 的链首就是最新的旧记录，链尾就是最早的旧记录（当然就像之前说的该 undo log 的节点可能是会 purge 线程清除掉，向图中的第一条 insert undo log，其实在事务提交之后有可能就被删除丢失了）



### 11.2.3 Read View(读视图)

事务进行`快照读`操作的时候生产的`读视图`(Read View)，在该事务执行的快照读的那一刻，会生成数据库系统当前的一个`快照`。

记录并维护系统当前`活跃事务的ID`(没有commit，当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以越新的事务，ID值越大)，是系统中当前不应该被`本事务`看到的`其他事务id列表`。

**作用：**

* Read View主要是用来做`可见性`判断的, 即当我们`某个事务`执行`快照读`的时候，对该记录创建一个Read View读视图，把它比作条件用来判断`当前事务`能够看到`哪个版本`的数据，既可能是当前`最新`的数据，也有可能是该行记录的undo log里面的`某个版本`的数

* **主要是将`要被修改的数据`的最新记录中的 `DB_TRX_ID`（即当前事务 ID ）取出来，与系统当前其他活跃事务的 ID 去对比（由 Read View 维护），如果 DB_TRX_ID 跟 Read View 的属性做了某些比较，不符合可见性，那就通过 `DB_ROLL_PTR 回滚指针`去取出 `Undo Log 中的 DB_TRX_ID 再比较`，即遍历链表的 DB_TRX_ID（从链首到链尾，即从最近的一次修改查起），直到找到满足特定条件的 DB_TRX_ID , 那么这个 DB_TRX_ID 所在的旧记录就是当前事务能看见的最新老版本**



**Read View几个属性**

- `trx_ids`: 当前系统活跃(`未提交`)事务版本号集合。
- `low_limit_id`: 创建当前read view 时“当前系统`最大事务版本号`+1”。
- `up_limit_id`: 创建当前read view 时“系统正处于活跃事务`最小版本号`”
- `creator_trx_id`: 创建当前read view的事务版本号；



**readview（重点）**

* 开始事务时创建readview，readView维护当前活动的事务id，即未提交的事务id，排序生成一个数组访问数据。然后获取版本链数据中的事务id（），对比readview：
* 首先比较 `DB_TRX_ID< up_limit_id` （比readview都小）, 如果小于，则当前事务能看到 `creator_trx_id` 所在的记录，如果大于等于进入下一个判断
* 接下来判断 `DB_TRX_ID>= low_limit_id` , 如果大于等于则代表 `DB_TRX_ID` 所在的记录在 `Read View` 生成后才出现的，那对当前事务肯定不可见，如果小于则进入下一个判断
* 判断 `DB_TRX_ID`  是否在活跃事务之中，**如果在**，则代表我 Read View 生成时刻，你这个事务还在活跃，还没有 Commit，你修改的数据，**当前事务也是看不见的**；**如果不在**，则说明，你这个事务在 Read View 生成之前就已经 Commit 了，你修改的结果，**当前事务是能看见的。**





**读已提交（RC）和可重复读（RR）的区别就在于它们生成ReadView的策略不同**。

* 读已提交隔离级别下的事务，**在每次查询的开始都会生成一个独立的ReadView**。所以在RC级别下的事务可以看到别的事务提交的更新的原因。
* 而可重复读（RR）隔离级别下的事务，则**在第一次读的时候生成一个ReadView**，之后的读都复用之前的ReadView



<font color='blue'>**MySQL是怎么解决幻读问题的？**</font>

**快照读的幻读是用MVCC解决的，当前的读的幻读是用gap lock(间隙锁)解决的**。MySQL 把行锁和间隙锁合并在一起，解决了并发写和幻读的问题，这个锁叫做 Next-Key锁。

```sql
当前读：SELECT * FROM child WHERE id> 100 FOR UPDATE;
```

在搜索和扫描index的时候使用的next-key locks可以避免幻读。也就是说的间隙锁

我们查询条件是a>100，所以会申请行锁中的next-key lock，是会对上面这个区间都加锁，这样其他事务不能往这两个区间插入数据，事务B会执行插入时会一直等待获取锁，直到事务A提交，释放行锁，事务B才有可能申请到锁，然后进行插入。这样就解决了幻读问题

# 十二、MySQL主从复制原理



> 好的博客：[MySQL主从复制读写分离，看这篇就够了！](https://blog.csdn.net/alitech2017/article/details/108241782?ops_request_misc=%7B%22request%5Fid%22%3A%22163247090216780269819112%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=163247090216780269819112&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~top_positive~default-1-108241782.pc_v2_rank_blog_default&utm_term=mysql主从复制&spm=1018.2226.3001.4450)
>
> [MySQL的主从复制](https://blog.csdn.net/y_zilong/article/details/116950207)

* **主从复制、读写分离**一般是一起使用的。目的很简单，就是**为了提高数据库的并发性能（master复制读写，slave负责读）**。
* 同时，随着业务量的扩展、如果是单机部署的MySQL，会导致I/O频率过高。采用**主从复制、读写分离可以提高数据库的可用性**。



<font color='blue'>**主从复制的原理：**</font>

* **slave的`IO线程`向master请求从指定的binlog日志文件的指定位置之后的`binlog`日志内容**
* salve从库连接master主库，Master有多少个slave就会创建多少个**binlog dump线程**。
* 当Master节点进行insert、update、delete操作时，会按顺序写入到binlog中。
* 当Master节点的binlog发生变化时，**binlog dump 线程会通知所有的salve节点，向其发送二进制事件日志**。
* **`I/O线程`接收到 binlog 内容后，将内容写入到本地的 `relay-log`（中继日志文件）**。
* **`SQL线程`读取I/O线程写入的`relay-log`，并且根据 relay-log 的内容对从数据库做对应的操作**。

![image-20210924164314430](https://gitee.com/jobim/blogimage/raw/master/img/20210924164314.png)



**主从复制三个线程：**

* 主节点：dump Thread：为每个Slave的I/O Thread启动一个dump线程，用于向其发送binary log events
* 从节点：I/O Thread：向Master请求二进制日志事件，并保存于中继日志中
* 从节点：SQL Thread：从中继日志中读取日志事件，在本地完成重放













**分库分表带来的问题？**

* **事务问题**

  如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价； 如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。

* **跨节点Join的问题**

  只要是进行切分，跨节点Join的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的id,根据这些id发起第二次请求得到关联数据。

* **跨节点的count,order by,group by以及聚合函数问题**

* 数据迁移，容量规划，扩容等问题

* **ID问题**

  我们往往直接使用数据库自增特性来生成主键ID，这样确实比较简单。而在分库分表的环境中，数据分布在不同的分片上，不能再借助数据库自增长特性直接生成，否则会造成不同分片上的数据表主键会重复

* **跨分片的排序分页**

  分页时需要按照指定字段进行排序。当排序字段就是分片字段的时候，我们通过分片规则可以比较容易定位到指定的分片，而当排序字段非分片字段的时候，情况就会变得比较复杂了。为了最终结果的准确性，我们需要在不同的分片节点中将数据进行排序并返回，并将不同分片返回的结果集进行汇总和再次排序，最后再返回给用户。



* 



6、数据库查询优化

3.数据库事务，然后问我mysql三个select不显式声明事务，他们每一条是个事务吗？这里记得有点不清楚，就是问不显式声明事务，select是一个事务吗
默认autocommit，每一条都是是一个事务




乐观锁在数据库中就是MVCC，悲观锁就是行锁和表锁。innodb支持行锁，在索引上加锁



16.mysql的调优怎么调优的？怎么看？



19.说一下索引机制，什么时候会用到索引？

20.你说like %不能用索引，我要是想用怎么办？你有什么办法吗？

21.慢查询什么时候会出现？你有什么解决办法吗？什么时候需要建立索引？创建索引的原则？



innodb支持事务，你知道是通过什么实现的吗？简单说一下



知道哪些数据库的优化，都说一下。

写SQL，一张表三个字段，学生id，课程id，成绩，查询每一个学生得分最高的那门课

